{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EE chula model4***\n",
    "\n",
    "# Implementation\n",
    "- number of layer = 2\n",
    "- number of hidden layer = 1\n",
    "- layer 1 = 4 units, activation function is tanh\n",
    "- loss = Mean Squared Error (MSE) = 0.064369\n",
    "- stop training when epochs about 1000 epochs\n",
    "\n",
    "# Feature\n",
    "measurement data of solar cell on the rooftop of EE building (capacity of 8kW) collected during Jan 2017-Jun 2018 through CUBEMS portal. \n",
    "\n",
    "- datetime\n",
    "- date\n",
    "- time\n",
    "- I is Solar irradiance (W/m2)\n",
    "- T is Temperature (oC*10)\n",
    "- UV is UV index (UV index*10)\n",
    "- WS is Wind speed (m/s*10)\n",
    "- RH is Relative humidity (%)\n",
    "- P is Solar power (W*min)\n",
    "\n",
    "# Result\n",
    "- MSE = 0.064369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link './data': File exists\n",
      "ln: failed to create symbolic link './out': File exists\n"
     ]
    }
   ],
   "source": [
    "!ln -s ../data/ ./\n",
    "!ln -s ../out/ ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train_data/dataset_rev4_train.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_train = train.datetime\n",
    "#remove object\n",
    "train = train.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_data/dataset_rev4_test.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_test = test.datetime\n",
    "#remove object\n",
    "test = test.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 13159\n",
      "Number of rows without outliers: 118427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train)\n",
    "y_noano = clf.predict(train)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "print(\"Number of rows without outliers:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "\n",
    "col_train_bis.remove('P')\n",
    "\n",
    "mat_train = np.matrix(train)\n",
    "mat_test  = np.matrix(test)\n",
    "\n",
    "mat_new = np.matrix(train.drop('P',axis = 1))\n",
    "mat_y = np.array(train.P).reshape((118427,1))\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y)\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "test  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training_set and prediction_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "COLUMNS = col_train #column train (x train)\n",
    "FEATURES = col_train_bis  #column train-label (x test)\n",
    "LABEL = \"P\"\n",
    "\n",
    "# Columns\n",
    "feature_cols = FEATURES #(x test)\n",
    "\n",
    "# Training set and Prediction set with the features to predict\n",
    "training_set = train[COLUMNS] #column train (x train)\n",
    "prediction_set = train.P # column P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create x_train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "training_set = pd.DataFrame(x_train, columns = FEATURES).merge(y_train, left_index = True, right_index = True)\n",
    "training_sub = training_set[col_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test, columns = [LABEL])\n",
    "testing_set = pd.DataFrame(x_test, columns = FEATURES).merge(y_test, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>T</th>\n",
       "      <th>UV</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108496</th>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.452675</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.279006</td>\n",
       "      <td>0.619369</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107582</th>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677928</td>\n",
       "      <td>0.188525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               I         T        UV        WS        RH         P\n",
       "108496  0.034862  0.452675  0.019608  0.279006  0.619369  0.016393\n",
       "107582  0.196330  0.539095  0.176471  0.000000  0.677928  0.188525"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_ndarray = data_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_df_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_df_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.452675</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.279006</td>\n",
       "      <td>0.619369</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677928</td>\n",
       "      <td>0.188525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.034862  0.452675  0.019608  0.279006  0.619369  0.016393\n",
       "1  0.196330  0.539095  0.176471  0.000000  0.677928  0.188525"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data_df.loc[:,0:4]\n",
    "label = data_df.loc[:,5:5]\n",
    "\n",
    "X = feature.values\n",
    "Y = label.values\n",
    "X_new = X.transpose()\n",
    "Y_new = Y.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 79346)\n",
      "(1, 79346)\n"
     ]
    }
   ],
   "source": [
    "print(X_new.shape)\n",
    "print(Y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/z9wmbWbmkWQZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f87b14438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_new[0, :], X_new[2, :]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dimension of variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (5, 79346)\n",
      "The shape of Y is: (1, 79346)\n",
      "I have m = 79346 training examples!\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "shape_X = X_new.shape\n",
    "shape_Y = Y_new.shape\n",
    "m = Y_new.size  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 5\n",
      "The size of the hidden layer is: n_h = 4\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X_new, Y_new)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: initialize_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-4.16757847e-03 -5.62668272e-04 -2.13619610e-02  1.64027081e-02\n",
      "  -1.79343559e-02]\n",
      " [-8.41747366e-03  5.02881417e-03 -1.24528809e-02 -1.05795222e-02\n",
      "  -9.09007615e-03]\n",
      " [ 5.51454045e-03  2.29220801e-02  4.15393930e-04 -1.11792545e-02\n",
      "   5.39058321e-03]\n",
      " [-5.96159700e-03 -1.91304965e-04  1.17500122e-02 -7.47870949e-03\n",
      "   9.02525097e-05]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00878108 -0.00156434  0.0025657  -0.00988779]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : (n_h, n_x) (4, 5)\n",
      "X : (n_x, m) (5, 79346)\n",
      "b1 : (n_h, 1) (4, 1)\n",
      "W2 : (n_y, n_h) (1, 4)\n",
      "b2 : (n_y, 1)(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"W1 : (n_h, n_x) \" + str(parameters[\"W1\"].shape))\n",
    "print(\"X : (n_x, m) \" + str(X_new.shape))\n",
    "print(\"b1 : (n_h, 1) \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 : (n_y, n_h) \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 : (n_y, 1)\" + str(parameters[\"b2\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)   \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0028493365085705182 -0.0028490476624799623 0.0001995557475699708 0.00019955574429407242\n"
     ]
    }
   ],
   "source": [
    "X_assess, parameters = X_new, parameters\n",
    "\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "change cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    \n",
    "    # Retrieve W1 and W2 from parameters\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    #logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1-A2))\n",
    "    ### END CODE HERE ###\n",
    "    #cost = -1/m * np.sum(logprobs) \n",
    "    \n",
    "    #cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    cost = sqrt(mean_squared_error(A2, Y))       \n",
    "        \n",
    "        \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.3505309753351035\n"
     ]
    }
   ],
   "source": [
    "A2, Y_assess, parameters = A2,Y_new,parameters\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: backward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2)))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[ 1.12767931e-03  1.30170913e-03  8.51595608e-04  3.43674568e-04\n",
      "   1.52887176e-03]\n",
      " [ 2.00943667e-04  2.31950600e-04  1.51751117e-04  6.12331118e-05\n",
      "   2.72432727e-04]\n",
      " [-3.29533018e-04 -3.80374516e-04 -2.48862617e-04 -1.00423867e-04\n",
      "  -4.46769764e-04]\n",
      " [ 1.27039383e-03  1.46638112e-03  9.59404883e-04  3.87121414e-04\n",
      "   1.72231530e-03]]\n",
      "db1 = [[ 0.00234409]\n",
      " [ 0.00041769]\n",
      " [-0.00068497]\n",
      " [ 0.0026406 ]]\n",
      "dW2 = [[ 5.17254806e-03  3.54131048e-03 -4.64898812e-03 -6.86904349e-05]]\n",
      "db2 = [[-0.26705729]]\n"
     ]
    }
   ],
   "source": [
    "parameters, cache, X_assess, Y_assess = parameters,cache,X_new,Y_new\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: update_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.00552079 -0.00212472 -0.02238388  0.0159903  -0.019769  ]\n",
      " [-0.00865861  0.00475047 -0.01263498 -0.010653   -0.009417  ]\n",
      " [ 0.00590998  0.02337853  0.00071403 -0.01105875  0.00592671]\n",
      " [-0.00748607 -0.00195096  0.01059873 -0.00794326 -0.00197653]]\n",
      "b1 = [[-0.00281291]\n",
      " [-0.00050123]\n",
      " [ 0.00082197]\n",
      " [-0.00316872]]\n",
      "W2 = [[-0.01498814 -0.00581391  0.00814449 -0.00980536]]\n",
      "b2 = [[0.32046875]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = parameters,grads\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADED FUNCTION: nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.350531\n",
      "Cost after iteration 1000: 0.064369\n",
      "W1 = [[-1.05038014  0.22429357 -0.24073736  0.0586456   0.22911081]\n",
      " [-0.6578439   0.05889285 -0.16347075 -0.00926632 -0.04216255]\n",
      " [ 0.60984732 -0.02522552  0.14256979 -0.01267842  0.03291563]\n",
      " [-0.36245535  0.020725   -0.06718232 -0.0079195  -0.01306803]]\n",
      "b1 = [[ 0.37545959]\n",
      " [ 0.09373869]\n",
      " [-0.08441285]\n",
      " [ 0.02996647]]\n",
      "W2 = [[-0.54671966 -0.3146287   0.29073578 -0.17230641]]\n",
      "b2 = [[0.36379793]]\n"
     ]
    }
   ],
   "source": [
    "X_assess, Y_assess = X_new,Y_new\n",
    "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=2000, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
