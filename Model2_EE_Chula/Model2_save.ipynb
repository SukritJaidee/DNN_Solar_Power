{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EE chula model2***\n",
    "\n",
    "# Implementation\n",
    "- number of layer = 4\n",
    "- number of hidden layer = 3\n",
    "- layer 1 = 64 units, activation function is relu\n",
    "- layer 2 = 64 units, activation function is relu\n",
    "- layer 3 = 64 units, activation function is relu\n",
    "- loss = Mean Squared Error (MSE) = 0.03\n",
    "- metrics = Mean Absolute Error (MAE) \n",
    "- optimizer = RMSPropOptimizer\n",
    "- kernel_initializer = uniform\n",
    "- stop training when the validation score doesn't improve. = about 500 epochs\n",
    "\n",
    "# Feature\n",
    "measurement data of solar cell on the rooftop of EE building (capacity of 8kW) collected during Jan 2017-Jun 2018 through CUBEMS portal. \n",
    "\n",
    "- datetime\n",
    "- date\n",
    "- time\n",
    "- I is Solar irradiance (W/m2)\n",
    "- T is Temperature (oC*10)\n",
    "- UV is UV index (UV index*10)\n",
    "- WS is Wind speed (m/s*10)\n",
    "- RH is Relative humidity (%)\n",
    "- P is Solar power (W*min)\n",
    "\n",
    "# Result\n",
    "- MSE = 0.03 on the test set\n",
    "\n",
    "\n",
    "# Note\n",
    "- loss=mean_squared_error,mean_absolute_error,mean_absolute_percentage_error,mean_squared_logarithmic_error,\n",
    "     squared_hinge,hinge,logcosh<br>\n",
    "- optimizer = sgd(Stochastic gradient descent optimizer),RMSProp optimizer,Adagrad optimizer,Adadelta  <br>                     optimizer,Adam,Adamax,Nadam,TFOptimizer\n",
    "- metrics = mae,acc<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train_data/dataset_rev4_train.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_train = train.datetime\n",
    "#remove object\n",
    "train = train.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_data/dataset_rev4_test.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_test = test.datetime\n",
    "#remove object\n",
    "test = test.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of train: (131586, 6)\n",
      "dimension of test: (16147, 5)\n"
     ]
    }
   ],
   "source": [
    "print('dimension of train:', train.shape)\n",
    "print('dimension of test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['I', 'T', 'UV', 'WS', 'RH', 'P']\n"
     ]
    }
   ],
   "source": [
    "print(\"features:\",list(train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 13159\n",
      "Number of rows without outliers: 118427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train)\n",
    "y_noano = clf.predict(train)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "print(\"Number of rows without outliers:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "When input data features have values with different ranges, each feature should be scaled independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "\n",
    "col_train_bis.remove('P')\n",
    "\n",
    "mat_train = np.matrix(train)\n",
    "mat_test  = np.matrix(test)\n",
    "\n",
    "mat_new = np.matrix(train.drop('P',axis = 1))\n",
    "mat_y = np.array(train.P).reshape((118427,1))\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y)\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "test  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training_set and prediction_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "COLUMNS = col_train #column train (x train)\n",
    "FEATURES = col_train_bis  #column train-label (x test)\n",
    "LABEL = \"P\"\n",
    "\n",
    "# Columns\n",
    "feature_cols = FEATURES #(x test)\n",
    "\n",
    "# Training set and Prediction set with the features to predict\n",
    "training_set = train[COLUMNS] #column train (x train)\n",
    "prediction_set = train.P # column P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_set))\n",
    "print(type(prediction_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create x_train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(test_data))\n",
    "print(type(train_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79346, 5)\n",
      "(39081, 5)\n",
      "(79346,)\n",
      "(39081,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.values\n",
    "test_data = test_data.values\n",
    "train_labels = train_labels.values\n",
    "test_labels = test_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(np.random.random(train_labels.shape))\n",
    "train_data = train_data[order]\n",
    "train_labels = train_labels[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (79346, 5)\n",
      "Testing set:  (39081, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: {}\".format(train_data.shape))  \n",
    "print(\"Testing set:  {}\".format(test_data.shape))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 64)                384       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,769\n",
      "Trainable params: 8,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Returns a short sequential model\n",
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(train_data.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "  \n",
    "  #optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model\n",
    "\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79346 samples, validate on 39081 samples\n",
      "Epoch 1/10\n",
      "79346/79346 [==============================] - 3s 38us/step - loss: 0.0063 - mean_absolute_error: 0.0457 - val_loss: 0.0046 - val_mean_absolute_error: 0.0456\n",
      "\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "Epoch 2/10\n",
      "79346/79346 [==============================] - 3s 33us/step - loss: 0.0037 - mean_absolute_error: 0.0359 - val_loss: 0.0037 - val_mean_absolute_error: 0.0354\n",
      "\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "Epoch 3/10\n",
      "79346/79346 [==============================] - 3s 33us/step - loss: 0.0037 - mean_absolute_error: 0.0351 - val_loss: 0.0045 - val_mean_absolute_error: 0.0451\n",
      "\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "Epoch 4/10\n",
      "79346/79346 [==============================] - 3s 33us/step - loss: 0.0036 - mean_absolute_error: 0.0345 - val_loss: 0.0035 - val_mean_absolute_error: 0.0330\n",
      "\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "Epoch 5/10\n",
      "79346/79346 [==============================] - 3s 33us/step - loss: 0.0036 - mean_absolute_error: 0.0342 - val_loss: 0.0035 - val_mean_absolute_error: 0.0322\n",
      "\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "Epoch 6/10\n",
      "79346/79346 [==============================] - 3s 43us/step - loss: 0.0035 - mean_absolute_error: 0.0340 - val_loss: 0.0036 - val_mean_absolute_error: 0.0346\n",
      "\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "Epoch 7/10\n",
      "79346/79346 [==============================] - 3s 36us/step - loss: 0.0035 - mean_absolute_error: 0.0339 - val_loss: 0.0035 - val_mean_absolute_error: 0.0335\n",
      "\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "Epoch 8/10\n",
      "79346/79346 [==============================] - 3s 34us/step - loss: 0.0035 - mean_absolute_error: 0.0338 - val_loss: 0.0035 - val_mean_absolute_error: 0.0351\n",
      "\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "Epoch 9/10\n",
      "79346/79346 [==============================] - 3s 33us/step - loss: 0.0035 - mean_absolute_error: 0.0336 - val_loss: 0.0036 - val_mean_absolute_error: 0.0343\n",
      "\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "Epoch 10/10\n",
      "79346/79346 [==============================] - 3s 34us/step - loss: 0.0035 - mean_absolute_error: 0.0335 - val_loss: 0.0034 - val_mean_absolute_error: 0.0319\n",
      "\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f01d4cd15f8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(train_data, train_labels,  epochs = 10, \n",
    "          validation_data = (test_data,test_labels),\n",
    "          callbacks = [cp_callback])  # pass callback to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  cp.ckpt.data-00000-of-00001  cp.ckpt.index\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39081/39081 [==============================] - 1s 15us/step\n",
      "Testing set Mean Abs Error: $   0.34\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "[loss, mae] = model.evaluate(test_data, test_labels)\n",
    "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the weights from the checkpoint, and re-evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39081/39081 [==============================] - 0s 13us/step\n",
      "Restored model, MAE:  0.03%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "[loss, mae] = model.evaluate(test_data, test_labels)\n",
    "print(\"Restored model, MAE: {:5.2f}%\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint callback options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f01d4d385c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # Save weights, every 5-epochs.\n",
    "    period=5)\n",
    "\n",
    "model = create_model()\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs = 50, callbacks = [cp_callback],\n",
    "          validation_data = (test_data,test_labels),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('training_2/cp-0030.ckpt'),\n",
       " PosixPath('training_2/cp-0035.ckpt'),\n",
       " PosixPath('training_2/cp-0040.ckpt'),\n",
       " PosixPath('training_2/cp-0045.ckpt'),\n",
       " PosixPath('training_2/cp-0050.ckpt')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# Sort the checkpoints by modification time.\n",
    "checkpoints = pathlib.Path(checkpoint_dir).glob(\"*.index\")\n",
    "checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "latest = str(checkpoints[-1])\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39081/39081 [==============================] - 1s 15us/step\n",
      "Restored model, MAE:  0.03%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "[loss, mae] = model.evaluate(test_data, test_labels)\n",
    "print(\"Restored model, MAE: {:5.2f}%\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39081/39081 [==============================] - 1s 17us/step\n",
      "Restored model, MAE:  0.03%\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# Restore the weights\n",
    "model = create_model()\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "[loss, mae] = model.evaluate(test_data, test_labels)\n",
    "print(\"Restored model, MAE: {:5.2f}%\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the entire model\n",
    "The entire model can be saved to a file that contains the weight values, the model's configuration, and even the optimizer's configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79346/79346 [==============================] - 4s 44us/step - loss: 0.0042 - mean_absolute_error: 0.0385\n",
      "Epoch 2/5\n",
      "79346/79346 [==============================] - 3s 36us/step - loss: 0.0036 - mean_absolute_error: 0.0353\n",
      "Epoch 3/5\n",
      "79346/79346 [==============================] - 3s 36us/step - loss: 0.0036 - mean_absolute_error: 0.0346\n",
      "Epoch 4/5\n",
      "79346/79346 [==============================] - 3s 36us/step - loss: 0.0035 - mean_absolute_error: 0.0341\n",
      "Epoch 5/5\n",
      "79346/79346 [==============================] - 3s 36us/step - loss: 0.0035 - mean_absolute_error: 0.0338\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_data, train_labels, epochs=5)\n",
    "# Save entire model to a HDF5 file\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 64)                384       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,769\n",
      "Trainable params: 8,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including weights and optimizer.\n",
    "new_model = keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39081/39081 [==============================] - 1s 15us/step\n",
      "Restored model, MAE:  0.03%\n"
     ]
    }
   ],
   "source": [
    "[loss, mae] = new_model.evaluate(test_data, test_labels)\n",
    "print(\"Restored model, MAE: {:5.2f}%\".format(mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
