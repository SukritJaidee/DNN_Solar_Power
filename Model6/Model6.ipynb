{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../data/ ./\n",
    "!ln -s ../out/ ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "from pylab import rcParams\n",
    "import matplotlib\n",
    "\n",
    "# Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train_data/dataset_rev4_train.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_train = train.datetime\n",
    "#remove object\n",
    "train = train.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_data/dataset_rev4_test.csv')\n",
    "#buffer datetime\n",
    "buffer_datetime_test = test.datetime\n",
    "#remove object\n",
    "test = test.select_dtypes(exclude=['object'])\n",
    "#replace misssing value\n",
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of train: (131586, 6)\n",
      "dimension of test: (16147, 5)\n"
     ]
    }
   ],
   "source": [
    "print('dimension of train:', train.shape)\n",
    "print('dimension of test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['I', 'T', 'UV', 'WS', 'RH', 'P']\n"
     ]
    }
   ],
   "source": [
    "print(\"features:\",list(train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 13159\n",
      "Number of rows without outliers: 118427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(max_samples = 100, random_state = 42)\n",
    "clf.fit(train)\n",
    "y_noano = clf.predict(train)\n",
    "y_noano = pd.DataFrame(y_noano, columns = ['Top'])\n",
    "y_noano[y_noano['Top'] == 1].index.values\n",
    "\n",
    "train = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\n",
    "train.reset_index(drop = True, inplace = True)\n",
    "print(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\n",
    "print(\"Number of rows without outliers:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "\n",
    "col_train_bis.remove('P')\n",
    "\n",
    "mat_train = np.matrix(train)\n",
    "mat_test  = np.matrix(test)\n",
    "\n",
    "mat_new = np.matrix(train.drop('P',axis = 1))\n",
    "mat_y = np.array(train.P).reshape((118427,1))\n",
    "\n",
    "prepro_y = MinMaxScaler()\n",
    "prepro_y.fit(mat_y)\n",
    "\n",
    "prepro = MinMaxScaler()\n",
    "prepro.fit(mat_train)\n",
    "\n",
    "prepro_test = MinMaxScaler()\n",
    "prepro_test.fit(mat_new)\n",
    "\n",
    "train = pd.DataFrame(prepro.transform(mat_train),columns = col_train)\n",
    "test  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_set, pred = False):\n",
    "    if pred == False:      \n",
    "        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "        labels = tf.constant(data_set[LABEL].values)      \n",
    "        return feature_cols, labels\n",
    "    if pred == True:\n",
    "        feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}      \n",
    "        return feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training_set and prediction_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features# List of \n",
    "COLUMNS = col_train #column train (x train)\n",
    "FEATURES = col_train_bis  #column train-label (x test)\n",
    "LABEL = \"P\"\n",
    "\n",
    "# Columns\n",
    "feature_cols = FEATURES #(x test)\n",
    "\n",
    "# Training set and Prediction set with the features to predict\n",
    "training_set = train[COLUMNS] #column train (x train)\n",
    "prediction_set = train.P # column P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_set))\n",
    "print(type(prediction_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create x_train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(training_set[FEATURES] , prediction_set, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79346, 5)\n",
      "(39081, 5)\n",
      "(79346,)\n",
      "(39081,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train, columns = [LABEL])\n",
    "training_set = pd.DataFrame(x_train, columns = FEATURES).merge(y_train, left_index = True, right_index = True)\n",
    "training_sub = training_set[col_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test, columns = [LABEL])\n",
    "testing_set = pd.DataFrame(x_test, columns = FEATURES).merge(y_test, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x_test =  (39081, 5)\n",
      "Dimension of y_test =  (39081, 1)\n",
      "Dimension of x_train =  (79346, 5)\n",
      "Dimension of y_train =  (79346, 1)\n"
     ]
    }
   ],
   "source": [
    "print( \"Dimension of x_test = \", x_test.shape )\n",
    "print( \"Dimension of y_test = \", y_test.shape )\n",
    "print( \"Dimension of x_train = \", x_train.shape )\n",
    "print( \"Dimension of y_train = \", y_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We scale the inputs to have mean 0 and standard variation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = preprocessing.StandardScaler( )\n",
    "#train_x = scaler.fit_transform( train_x )\n",
    "#test_x  = scaler.fit_transform( test_x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We verify that we have 5 features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features =  5\n"
     ]
    }
   ],
   "source": [
    "numFeatures =  x_train.shape[1] \n",
    "print( \"number of features = \", numFeatures )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Input & Output Place-Holders</h2>\n",
    "Define 2 place holders to the graph, one for the inputs one for the outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"IO\"):\n",
    "    inputs = tf.placeholder(tf.float32, [None, numFeatures], name=\"X\")\n",
    "    outputs = tf.placeholder(tf.float32, [None, 1], name=\"Yhat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the Coeffs for the Layers</h2>\n",
    "For each layer the input vector will be multiplied by a matrix $h$ of dim $n$ x $m$, where $n$ is the dimension of the input vector and $m$ the dimention of the output vector.   Then a bias vector of dimension $m$ is added to the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"LAYER\"):\n",
    "    # network architecture\n",
    "    Layers = [numFeatures, 52, 39, 26, 13, 1]\n",
    "    \n",
    "    h1   = tf.Variable(tf.random_normal([Layers[0], Layers[1]], 0, 0.1, dtype=tf.float32), name=\"h1\")\n",
    "    h2   = tf.Variable(tf.random_normal([Layers[1], Layers[2]], 0, 0.1, dtype=tf.float32), name=\"h2\")\n",
    "    h3   = tf.Variable(tf.random_normal([Layers[2], Layers[3]], 0, 0.1, dtype=tf.float32), name=\"h3\")\n",
    "    h4   = tf.Variable(tf.random_normal([Layers[3], Layers[4]], 0, 0.1, dtype=tf.float32), name=\"h4\")\n",
    "    hout = tf.Variable(tf.random_normal([Layers[4], Layers[5]], 0, 0.1, dtype=tf.float32), name=\"hout\")\n",
    "\n",
    "    b1   = tf.Variable(tf.random_normal([Layers[1]], 0, 0.1, dtype=tf.float32 ), name=\"b1\" )\n",
    "    b2   = tf.Variable(tf.random_normal([Layers[2]], 0, 0.1, dtype=tf.float32 ), name=\"b2\" )\n",
    "    b3   = tf.Variable(tf.random_normal([Layers[3]], 0, 0.1, dtype=tf.float32 ), name=\"b3\" )\n",
    "    b4   = tf.Variable(tf.random_normal([Layers[4]], 0, 0.1, dtype=tf.float32 ), name=\"b4\" )\n",
    "    bout = tf.Variable(tf.random_normal([Layers[5]], 0, 0.1, dtype=tf.float32 ), name=\"bout\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the Layer operations as a Python funtion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model( inputs, layers ):\n",
    "    [h1, b1, h2, b2, h3, b3, hout, bout] = layers\n",
    "    y1 = tf.add( tf.matmul(inputs, h1), b1 )\n",
    "    y1 = tf.nn.relu( y1 )\n",
    "       \n",
    "    y2 = tf.add( tf.matmul(y1, h2), b2 )\n",
    "    y2 = tf.nn.relu( y2 )\n",
    "\n",
    "    y3 = tf.add( tf.matmul(y2, h3), b3 )\n",
    "    y3 = tf.nn.relu( y3 )\n",
    "\n",
    "    y4 = tf.add( tf.matmul(y3, h4), b4 )\n",
    "    y4 = tf.nn.relu( y4 )\n",
    "\n",
    "    yret  = tf.matmul(y4, hout) + bout \n",
    "    return yret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the operations that are performed</h2>\n",
    "We define what happens to the inputs (x), when they are provided, and what we do with \n",
    "the outputs of the layers (compare them to the y values), and the type of minimization \n",
    "that must be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    learning_rate = 0.50\n",
    "    yout = model( inputs, [h1, b1, h2, b2, h3, b3, hout, bout] )\n",
    "    \n",
    "    cost_op = tf.reduce_mean( tf.pow( yout - outputs, 2 ))\n",
    "    #cost_op = tf.reduce_sum( tf.pow( yout - outputs, 2 ))\n",
    "    #cost_op =  tf.reduce_mean(-tf.reduce_sum( yout * tf.log( outputs ) ) )\n",
    "\n",
    "    #train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_op)\n",
    "    #train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( cost_op )\n",
    "    train_op = tf.train.AdagradOptimizer( learning_rate=learning_rate ).minimize( cost_op )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the Model</h2>\n",
    "We are now ready to go through many sessions, and in each one train the model.  Here we train on the whole x-train and y-train data, rather than batching into smaller groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0 - Error: 0.1747\n",
      "Epoch: 1000 - Error: 0.0038\n",
      "Epoch: 2000 - Error: 0.0037\n",
      "Epoch: 3000 - Error: 0.0036\n",
      "Epoch: 4000 - Error: 0.0036\n",
      "Epoch: 5000 - Error: 0.0036\n",
      "Epoch: 6000 - Error: 0.0036\n",
      "Epoch: 7000 - Error: 0.0036\n",
      "Epoch: 8000 - Error: 0.0036\n",
      "Epoch: 9000 - Error: 0.0036\n",
      "Epoch: 10000 - Error: 0.0036\n",
      "Epoch: 11000 - Error: 0.0036\n",
      "Epoch: 12000 - Error: 0.0036\n",
      "Epoch: 13000 - Error: 0.0036\n",
      "Epoch: 14000 - Error: 0.0036\n",
      "Epoch: 15000 - Error: 0.0035\n",
      "Epoch: 16000 - Error: 0.0035\n",
      "Epoch: 17000 - Error: 0.0035\n",
      "Epoch: 18000 - Error: 0.0035\n",
      "Epoch: 19000 - Error: 0.0035\n",
      "Epoch: 20000 - Error: 0.0035\n",
      "Epoch: 21000 - Error: 0.0035\n",
      "Epoch: 22000 - Error: 0.0035\n",
      "Epoch: 23000 - Error: 0.0035\n",
      "Epoch: 24000 - Error: 0.0034\n",
      "Epoch: 25000 - Error: 0.0034\n",
      "Epoch: 26000 - Error: 0.0034\n",
      "Epoch: 27000 - Error: 0.0034\n",
      "Epoch: 28000 - Error: 0.0034\n",
      "Epoch: 29000 - Error: 0.0034\n",
      "Epoch: 30000 - Error: 0.0034\n",
      "Epoch: 31000 - Error: 0.0034\n",
      "Epoch: 32000 - Error: 0.0034\n",
      "Epoch: 33000 - Error: 0.0034\n",
      "Epoch: 34000 - Error: 0.0034\n",
      "Epoch: 35000 - Error: 0.0034\n",
      "Epoch: 36000 - Error: 0.0034\n",
      "Epoch: 37000 - Error: 0.0034\n",
      "Epoch: 38000 - Error: 0.0034\n",
      "Epoch: 39000 - Error: 0.0033\n",
      "Epoch: 40000 - Error: 0.0033\n",
      "Epoch: 41000 - Error: 0.0033\n",
      "Epoch: 42000 - Error: 0.0033\n",
      "Epoch: 44000 - Error: 0.0033\n",
      "Epoch: 45000 - Error: 0.0033\n",
      "Epoch: 46000 - Error: 0.0033\n",
      "Epoch: 47000 - Error: 0.0033\n",
      "Epoch: 48000 - Error: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# define variables/constants that control the training\n",
    "epoch = 0\n",
    "last_cost = 0\n",
    "#max_epochs = 100\n",
    "max_epochs = 50000\n",
    "tolerance = 1e-6\n",
    "\n",
    "print( \"Beginning Training\" )\n",
    "\n",
    "sess = tf.Session() # Create TensorFlow session\n",
    "with sess.as_default():\n",
    "    \n",
    "    # initialize the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # start training until we stop, either because we've reached the max\n",
    "    # number of epochs, or successive errors are close enough to each other\n",
    "    # (less than tolerance)\n",
    "    \n",
    "    costs = []\n",
    "    epochs= []\n",
    "    while True:\n",
    "        # Do the training\n",
    "        sess.run( train_op, feed_dict={inputs: x_train, outputs: y_train} )\n",
    "            \n",
    "        # Update the user every 1000 epochs\n",
    "        if epoch % 1000==0:\n",
    "            cost = sess.run(cost_op, feed_dict={inputs: x_train, outputs: y_train})\n",
    "            costs.append( cost )\n",
    "            epochs.append( epoch )\n",
    "            \n",
    "            print( \"Epoch: %d - Error: %.4f\" %(epoch, cost) )\n",
    "            \n",
    "            # time to stop?\n",
    "            if epoch > max_epochs :\n",
    "                # or abs(last_cost - cost) < tolerance:\n",
    "                print( \"STOP!\" )\n",
    "                break\n",
    "            last_cost = cost\n",
    "            \n",
    "        epoch += 1\n",
    "    \n",
    "    # we're done...\n",
    "    # print some statistics...\n",
    "    \n",
    "    print( \"Test Cost =\", sess.run(cost_op, feed_dict={inputs: x_test, outputs: y_test}) )\n",
    "\n",
    "    # compute the predicted output for test_x\n",
    "    pred_y = sess.run( yout, feed_dict={inputs: x_test, outputs: y_test} )\n",
    "    \n",
    "    print( \"\\nPrediction\\nreal\\tpredicted\" )\n",
    "    for (y, yHat ) in list(zip( y_test.values, pred_y ))[0:10]:\n",
    "        print( \"%1.1f\\t%1.1f\" % (y, yHat ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))\n",
    "print(type(pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>R2 score</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error =  0.003318053385004916\n",
      "r2 score (coef determination) =  0.9349347743494992\n"
     ]
    }
   ],
   "source": [
    "r2 =  metrics.r2_score(y_test, pred_y) \n",
    "print( \"mean squared error = \", metrics.mean_squared_error(y_test, pred_y))\n",
    "print( \"r2 score (coef determination) = \", metrics.r2_score(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot Prediction vs. Real Housing Price</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot Cost vs Epochs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGmM4SJLG/D9fSCZJBrdOlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe633e54358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = plt.figure()\n",
    "    plt.semilogy( epochs, costs )\n",
    "    plt.xlabel( \"Epochs\" )\n",
    "    plt.ylabel( \"Cost\" )\n",
    "    plt.title( \"Cost vs. Epochs\")\n",
    "    plt.show()\n",
    "    fig.savefig('CostVsEpochs.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
